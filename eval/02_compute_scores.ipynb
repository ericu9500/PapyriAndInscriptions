{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c13db4-a215-4e29-a197-47b735668c5a",
   "metadata": {},
   "source": [
    "These three cells compute test scores from the files in the folder results/, generated with 45 beams on A40s 45 GB memory (2–7 hours per process) and A100s 80GB (1-2 hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd8cd78-523e-4e59-95c0-fd92c0e8e30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tCER\tTop-1 Accuracy\tTop-20 Accuracy\n",
      "Epigr_1_Llama-3.1-8B-Instruct\t22.5%\t60.9%\t77.5%\n",
      "Papy_1_Llama-3.1-8B-Instruct\t16.3%\t71.3%\t85.0%\n"
     ]
    }
   ],
   "source": [
    "# Compute scores for text restoration\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r\"[0· ]\", \"\", text)\n",
    "\n",
    "def truncate_to_length(text, target_length):\n",
    "    valid_text = re.sub(r\"[0· ]\", \"\", text)\n",
    "    if len(valid_text) <= target_length:\n",
    "        return valid_text\n",
    "    truncated_valid_text = valid_text[:target_length]\n",
    "    reconstructed_text = \"\"\n",
    "    valid_count = 0\n",
    "    for char in text:\n",
    "        if char not in \"0· \":\n",
    "            if valid_count < target_length:\n",
    "                reconstructed_text += char\n",
    "                valid_count += 1\n",
    "        else:\n",
    "            reconstructed_text += char\n",
    "    return reconstructed_text\n",
    "\n",
    "def calculate_cer(real, predicted):\n",
    "    real_normalized = normalize_text(real)\n",
    "    predicted_normalized = normalize_text(predicted)\n",
    "    return (1 - SequenceMatcher(None, real_normalized, predicted_normalized).ratio()) * 100\n",
    "\n",
    "def process_file(file_path):\n",
    "    total_cer = 0\n",
    "    rank_1_matches = 0\n",
    "    real_in_top_20 = 0\n",
    "    total_lines = 0\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            real_response = entry[\"real_response\"]\n",
    "            rank_1_prediction = entry[\"predictions\"][\"rank_1\"]\n",
    "            real_response_normalized = normalize_text(real_response)\n",
    "            target_length = len(real_response_normalized)\n",
    "            rank_1_prediction_truncated = truncate_to_length(rank_1_prediction, target_length)\n",
    "            cer = calculate_cer(real_response, rank_1_prediction_truncated)\n",
    "            total_cer += cer\n",
    "            if real_response_normalized == normalize_text(rank_1_prediction_truncated):\n",
    "                rank_1_matches += 1\n",
    "            predictions = [truncate_to_length(pred, target_length) for pred in entry[\"predictions\"].values()]\n",
    "            predictions_normalized = [normalize_text(pred) for pred in predictions]\n",
    "            if real_response_normalized in predictions_normalized:\n",
    "                real_in_top_20 += 1\n",
    "            total_lines += 1\n",
    "    avg_cer = total_cer / total_lines\n",
    "    rank_1_match_percentage = (rank_1_matches / total_lines) * 100\n",
    "    real_in_top_20_percentage = (real_in_top_20 / total_lines) * 100\n",
    "    return avg_cer, rank_1_match_percentage, real_in_top_20_percentage\n",
    "\n",
    "def main():\n",
    "    directory_path = \"results\"\n",
    "    pattern = r\"(.+?)_text\\.jsonl\"\n",
    "    print(\"Model\\tCER\\tTop-1 Accuracy\\tTop-20 Accuracy\")\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if re.match(pattern, filename):\n",
    "            match = re.match(pattern, filename)\n",
    "            model = match.group(1)  # Extract only the model part\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            avg_cer, rank_1_match_percentage, real_in_top_20_percentage = process_file(file_path)\n",
    "            print(f\"{model}\\t{avg_cer:.1f}%\\t{rank_1_match_percentage:.1f}%\\t{real_in_top_20_percentage:.1f}%\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b26d99-2d22-4aaf-98eb-8f17a5362bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tTop-1 accuracy\tTop-3 accuracy\n",
      "Papy_1_Llama-3.1-8B-Instruct\t66.5%\t80.0%\n",
      "Epigr_1_Llama-3.1-8B-Instruct\t75.0%\t83.7%\n"
     ]
    }
   ],
   "source": [
    "# Compute scores for geographical attribution\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r\"[\\. /·,]\", \"\", text)\n",
    "\n",
    "def levenshtein_similarity(real, predicted):\n",
    "    real_truncated = real[:10]\n",
    "    predicted_truncated = predicted[:10]\n",
    "    real_normalized = normalize_text(real_truncated)\n",
    "    predicted_normalized = normalize_text(predicted_truncated)\n",
    "    matcher = SequenceMatcher(None, real_normalized, predicted_normalized)\n",
    "    return matcher.ratio() * 100\n",
    "\n",
    "def process_file(file_path):\n",
    "    rank_1_within_90 = 0\n",
    "    rank_1_3_within_90 = 0\n",
    "    total_lines = 0\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            real_response = entry[\"real_response\"]\n",
    "            rank_1_prediction = entry[\"predictions\"][\"rank_1\"]\n",
    "            rank_2_prediction = entry[\"predictions\"][\"rank_2\"]\n",
    "            rank_3_prediction = entry[\"predictions\"][\"rank_3\"]\n",
    "            if levenshtein_similarity(real_response, rank_1_prediction) >= 90:\n",
    "                rank_1_within_90 += 1\n",
    "            if (levenshtein_similarity(real_response, rank_1_prediction) >= 90 or\n",
    "                levenshtein_similarity(real_response, rank_2_prediction) >= 90 or\n",
    "                levenshtein_similarity(real_response, rank_3_prediction) >= 90):\n",
    "                rank_1_3_within_90 += 1\n",
    "            total_lines += 1\n",
    "    return rank_1_within_90, rank_1_3_within_90, total_lines\n",
    "\n",
    "def main():\n",
    "    directory_path = \"results/\"\n",
    "    pattern = r\"(.+?)_place\\.jsonl\"\n",
    "    print(\"Model\\tTop-1 accuracy\\tTop-3 accuracy\")\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if re.match(pattern, filename):\n",
    "            match = re.match(pattern, filename)\n",
    "            model = match.group(1)\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            rank_1_within_90, rank_1_3_within_90, total_lines = process_file(file_path)\n",
    "            rank_1_within_90_percentage = (rank_1_within_90 / total_lines) * 100 if total_lines > 0 else 0\n",
    "            rank_1_3_within_90_percentage = (rank_1_3_within_90 / total_lines) * 100 if total_lines > 0 else 0\n",
    "            print(f\"{model}\\t{rank_1_within_90_percentage:.1f}%\\t{rank_1_3_within_90_percentage:.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ccec48-0c66-44d1-8fe1-fd76df6bf568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tAverage Distance from span\tMedian distance\n",
      "Papy_1_Llama-3.1-8B-Instruct\t21.72 years\t0.00 years\n",
      "Epigr_1_Llama-3.1-8B-Instruct\t26.22 years\t1.00 years\n"
     ]
    }
   ],
   "source": [
    "# Compute scores for dating\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def calculate_date_range(date):\n",
    "    match = re.match(r\"(-?\\d+)±(\\d+)\", date)\n",
    "    if match:\n",
    "        base, margin = int(match.group(1)), int(match.group(2))\n",
    "        return base - margin, base + margin\n",
    "    elif date.endswith('+'):\n",
    "        base = int(date[:-1])\n",
    "        return base, base + 25\n",
    "    elif date.endswith('-'):\n",
    "        base = int(date[:-1])\n",
    "        return base - 25, base\n",
    "    return None\n",
    "\n",
    "def clean_prediction(prediction):\n",
    "    cleaned = prediction.replace(\"\\n\", \"\").replace(\"assistant\", \"\").strip()\n",
    "    try:\n",
    "        return convert_prediction_format(cleaned)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def convert_prediction_format(prediction):\n",
    "    \"\"\"Convert prediction to the desired format.\"\"\"\n",
    "    match = re.match(r\"(-?\\d+)±\\d+\", prediction)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "\n",
    "    match = re.match(r\"(-?\\d+)\\+\", prediction)  \n",
    "    if match:\n",
    "        return int(match.group(1)) + 12  \n",
    "\n",
    "    match = re.match(r\"(-?\\d+)\\-\", prediction) \n",
    "    if match:\n",
    "        return int(match.group(1)) - 12 \n",
    "\n",
    "    try:\n",
    "        return int(prediction)\n",
    "    except ValueError:\n",
    "        return None \n",
    "\n",
    "def calculate_distance(real_range, predicted):\n",
    "    \"\"\"Calculate the distance between the predicted date and the real date range.\"\"\"\n",
    "    real_start, real_end = real_range\n",
    "\n",
    "    if predicted is None:\n",
    "        return None\n",
    "\n",
    "    if real_start <= predicted <= real_end:\n",
    "        return 0 \n",
    "    return min(abs(real_start - predicted), abs(real_end - predicted))\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"Process a single JSONL file and return the list of distances.\"\"\"\n",
    "    distances = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                real_response = entry[\"real_response\"]\n",
    "                rank_1_prediction = entry[\"predictions\"][\"rank_1\"]\n",
    "\n",
    "                # Calculate the real range\n",
    "                real_range = calculate_date_range(real_response)\n",
    "                if real_range is None:\n",
    "                    continue\n",
    "\n",
    "                # Clean and convert rank 1 prediction to integer\n",
    "                rank_1_prediction_cleaned = clean_prediction(rank_1_prediction)\n",
    "\n",
    "                # Calculate the distance for the rank 1 prediction\n",
    "                distance = calculate_distance(real_range, rank_1_prediction_cleaned)\n",
    "                if distance is not None:\n",
    "                    distances.append(distance)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON line: {line.strip()}\")\n",
    "                continue\n",
    "\n",
    "    return distances\n",
    "\n",
    "def main():\n",
    "    directory_path = \"results/\"\n",
    "    pattern = r\"(.+?)_date\\.jsonl\"\n",
    "\n",
    "    print(\"Model\\tAverage Distance from span\\tMedian distance\")\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if re.match(pattern, filename):\n",
    "            match = re.match(pattern, filename)\n",
    "            model = match.group(1)  \n",
    "\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            distances = process_file(file_path)\n",
    "\n",
    "            if distances:\n",
    "                avg_accuracy = np.mean(distances)\n",
    "                median_accuracy = np.median(distances)\n",
    "\n",
    "                print(f\"{model}\\t{avg_accuracy:.2f} years\\t{median_accuracy:.2f} years\")\n",
    "            else:\n",
    "                print(f\"{model}\\tNo valid data\\tNo valid data\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MarcoPolo",
   "language": "python",
   "name": "eric_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
