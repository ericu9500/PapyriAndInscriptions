{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cd8cd78-523e-4e59-95c0-fd92c0e8e30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tCER\tTop-1 Accuracy\tTop-20 Accuracy\n",
      "fixed_DDbDP-minemerged1105_reconstructer_1\t16.3%\t71.3%\t85.0%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r\"[0· ]\", \"\", text)\n",
    "\n",
    "def truncate_to_length(text, target_length):\n",
    "    valid_text = re.sub(r\"[0· ]\", \"\", text)\n",
    "    if len(valid_text) <= target_length:\n",
    "        return valid_text\n",
    "    truncated_valid_text = valid_text[:target_length]\n",
    "    reconstructed_text = \"\"\n",
    "    valid_count = 0\n",
    "    for char in text:\n",
    "        if char not in \"0· \":\n",
    "            if valid_count < target_length:\n",
    "                reconstructed_text += char\n",
    "                valid_count += 1\n",
    "        else:\n",
    "            reconstructed_text += char\n",
    "    return reconstructed_text\n",
    "\n",
    "def calculate_cer(real, predicted):\n",
    "    real_normalized = normalize_text(real)\n",
    "    predicted_normalized = normalize_text(predicted)\n",
    "    return (1 - SequenceMatcher(None, real_normalized, predicted_normalized).ratio()) * 100\n",
    "\n",
    "def process_file(file_path):\n",
    "    total_cer = 0\n",
    "    rank_1_matches = 0\n",
    "    real_in_top_20 = 0\n",
    "    total_lines = 0\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            real_response = entry[\"real_response\"]\n",
    "            rank_1_prediction = entry[\"predictions\"][\"rank_1\"]\n",
    "            real_response_normalized = normalize_text(real_response)\n",
    "            target_length = len(real_response_normalized)\n",
    "            rank_1_prediction_truncated = truncate_to_length(rank_1_prediction, target_length)\n",
    "            cer = calculate_cer(real_response, rank_1_prediction_truncated)\n",
    "            total_cer += cer\n",
    "            if real_response_normalized == normalize_text(rank_1_prediction_truncated):\n",
    "                rank_1_matches += 1\n",
    "            predictions = [truncate_to_length(pred, target_length) for pred in entry[\"predictions\"].values()]\n",
    "            predictions_normalized = [normalize_text(pred) for pred in predictions]\n",
    "            if real_response_normalized in predictions_normalized:\n",
    "                real_in_top_20 += 1\n",
    "            total_lines += 1\n",
    "    avg_cer = total_cer / total_lines\n",
    "    rank_1_match_percentage = (rank_1_matches / total_lines) * 100\n",
    "    real_in_top_20_percentage = (real_in_top_20 / total_lines) * 100\n",
    "    return avg_cer, rank_1_match_percentage, real_in_top_20_percentage\n",
    "\n",
    "def main():\n",
    "    directory_path = \"output_efter_fix/\"\n",
    "    pattern = r\"eds_test_pap_def_eds_([^\\.]+)\\.jsonl\"\n",
    "    print(\"Model\\tCER\\tTop-1 Accuracy\\tTop-20 Accuracy\")\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if re.match(pattern, filename):\n",
    "            match = re.match(pattern, filename)\n",
    "            model = match.group(1)  # Extract only the model part\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            avg_cer, rank_1_match_percentage, real_in_top_20_percentage = process_file(file_path)\n",
    "            print(f\"{model}\\t{avg_cer:.1f}%\\t{rank_1_match_percentage:.1f}%\\t{real_in_top_20_percentage:.1f}%\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "432be18d-f25b-4971-b5a2-c533531e980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tCER\tTop-1 Accuracy\tTop-20 Accuracy\n",
      "fixed_quantized_DDbDP_text\t16.3%\t71.2%\t84.1%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r\"[0· ]\", \"\", text)\n",
    "\n",
    "def truncate_to_length(text, target_length):\n",
    "    valid_text = re.sub(r\"[0· ]\", \"\", text)\n",
    "    if len(valid_text) <= target_length:\n",
    "        return valid_text\n",
    "    truncated_valid_text = valid_text[:target_length]\n",
    "    reconstructed_text = \"\"\n",
    "    valid_count = 0\n",
    "    for char in text:\n",
    "        if char not in \"0· \":\n",
    "            if valid_count < target_length:\n",
    "                reconstructed_text += char\n",
    "                valid_count += 1\n",
    "        else:\n",
    "            reconstructed_text += char\n",
    "    return reconstructed_text\n",
    "\n",
    "def calculate_cer(real, predicted):\n",
    "    real_normalized = normalize_text(real)\n",
    "    predicted_normalized = normalize_text(predicted)\n",
    "    return (1 - SequenceMatcher(None, real_normalized, predicted_normalized).ratio()) * 100\n",
    "\n",
    "def process_file(file_path):\n",
    "    total_cer = 0\n",
    "    rank_1_matches = 0\n",
    "    real_in_top_20 = 0\n",
    "    total_lines = 0\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            real_response = entry[\"real_response\"]\n",
    "            rank_1_prediction = entry[\"predictions\"][\"rank_1\"]\n",
    "            real_response_normalized = normalize_text(real_response)\n",
    "            target_length = len(real_response_normalized)\n",
    "            rank_1_prediction_truncated = truncate_to_length(rank_1_prediction, target_length)\n",
    "            cer = calculate_cer(real_response, rank_1_prediction_truncated)\n",
    "            total_cer += cer\n",
    "            if real_response_normalized == normalize_text(rank_1_prediction_truncated):\n",
    "                rank_1_matches += 1\n",
    "            predictions = [truncate_to_length(pred, target_length) for pred in entry[\"predictions\"].values()]\n",
    "            predictions_normalized = [normalize_text(pred) for pred in predictions]\n",
    "            if real_response_normalized in predictions_normalized:\n",
    "                real_in_top_20 += 1\n",
    "            total_lines += 1\n",
    "    avg_cer = total_cer / total_lines\n",
    "    rank_1_match_percentage = (rank_1_matches / total_lines) * 100\n",
    "    real_in_top_20_percentage = (real_in_top_20 / total_lines) * 100\n",
    "    return avg_cer, rank_1_match_percentage, real_in_top_20_percentage\n",
    "\n",
    "def main():\n",
    "    directory_path = \"quant/\"\n",
    "    pattern = r\"eds_test_pap_def_eds_([^\\.]+)\\.jsonl\"\n",
    "    print(\"Model\\tCER\\tTop-1 Accuracy\\tTop-20 Accuracy\")\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if re.match(pattern, filename):\n",
    "            match = re.match(pattern, filename)\n",
    "            model = match.group(1)\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            avg_cer, rank_1_match_percentage, real_in_top_20_percentage = process_file(file_path)\n",
    "            print(f\"{model}\\t{avg_cer:.1f}%\\t{rank_1_match_percentage:.1f}%\\t{real_in_top_20_percentage:.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65b26d99-2d22-4aaf-98eb-8f17a5362bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tRank 1 within 90% Levenshtein\tRank 1-3 within 90% Levenshtein\n",
      "quantized_DDbDP_places\t56.0%\t68.9%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r\"[\\. /·,]\", \"\", text)\n",
    "\n",
    "def levenshtein_similarity(real, predicted):\n",
    "    real_truncated = real[:10]\n",
    "    predicted_truncated = predicted[:10]\n",
    "    real_normalized = normalize_text(real_truncated)\n",
    "    predicted_normalized = normalize_text(predicted_truncated)\n",
    "    matcher = SequenceMatcher(None, real_normalized, predicted_normalized)\n",
    "    return matcher.ratio() * 100\n",
    "\n",
    "def process_file(file_path):\n",
    "    rank_1_within_90 = 0\n",
    "    rank_1_3_within_90 = 0\n",
    "    total_lines = 0\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            real_response = entry[\"real_response\"]\n",
    "            rank_1_prediction = entry[\"predictions\"][\"rank_1\"]\n",
    "            rank_2_prediction = entry[\"predictions\"][\"rank_2\"]\n",
    "            rank_3_prediction = entry[\"predictions\"][\"rank_3\"]\n",
    "            if levenshtein_similarity(real_response, rank_1_prediction) >= 90:\n",
    "                rank_1_within_90 += 1\n",
    "            if (levenshtein_similarity(real_response, rank_1_prediction) >= 90 or\n",
    "                levenshtein_similarity(real_response, rank_2_prediction) >= 90 or\n",
    "                levenshtein_similarity(real_response, rank_3_prediction) >= 90):\n",
    "                rank_1_3_within_90 += 1\n",
    "            total_lines += 1\n",
    "    return rank_1_within_90, rank_1_3_within_90, total_lines\n",
    "\n",
    "def main():\n",
    "    directory_path = \"quant/\"\n",
    "    pattern = r\"places_test_pap_def_places_exact_([^\\.]+)\\.jsonl\"\n",
    "    print(\"Model\\tRank 1 within 90% Levenshtein\\tRank 1-3 within 90% Levenshtein\")\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if re.match(pattern, filename):\n",
    "            match = re.match(pattern, filename)\n",
    "            model = match.group(1)\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            rank_1_within_90, rank_1_3_within_90, total_lines = process_file(file_path)\n",
    "            rank_1_within_90_percentage = (rank_1_within_90 / total_lines) * 100 if total_lines > 0 else 0\n",
    "            rank_1_3_within_90_percentage = (rank_1_3_within_90 / total_lines) * 100 if total_lines > 0 else 0\n",
    "            print(f\"{model}\\t{rank_1_within_90_percentage:.1f}%\\t{rank_1_3_within_90_percentage:.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0cd8aca8-8d0d-4fa7-b131-89bc9efa593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tRank 1 within 90% Levenshtein\tRank 1-3 within 90% Levenshtein\n",
      "exact_allTies\t56.0%\t67.4%\n",
      "exact_DDbDP_geographer_5\t66.4%\t79.9%\n",
      "DDbDP_4_NO_INPUT\t62.3%\t77.9%\n",
      "DDbDPPHI_3_NO_INPUT\t59.8%\t77.0%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r\"[\\. /·,\\\\]\", \"\", text)\n",
    "\n",
    "def levenshtein_similarity(real, predicted):\n",
    "    real_truncated = real[:12]\n",
    "    predicted_truncated = predicted[:12]\n",
    "    real_normalized = normalize_text(real_truncated)\n",
    "    predicted_normalized = normalize_text(predicted_truncated)\n",
    "    matcher = SequenceMatcher(None, real_normalized, predicted_normalized)\n",
    "    return matcher.ratio() * 100\n",
    "\n",
    "def process_file(file_path):\n",
    "    rank_1_within_90 = 0\n",
    "    rank_1_3_within_90 = 0\n",
    "    total_lines = 0\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            real_response = entry[\"real_response\"]\n",
    "            rank_1_prediction = entry[\"predictions\"][\"rank_1\"]\n",
    "            rank_2_prediction = entry[\"predictions\"][\"rank_2\"]\n",
    "            rank_3_prediction = entry[\"predictions\"][\"rank_3\"]\n",
    "            if levenshtein_similarity(real_response, rank_1_prediction) >= 90:\n",
    "                rank_1_within_90 += 1\n",
    "            if (levenshtein_similarity(real_response, rank_1_prediction) >= 90 or\n",
    "                levenshtein_similarity(real_response, rank_2_prediction) >= 90 or\n",
    "                levenshtein_similarity(real_response, rank_3_prediction) >= 90):\n",
    "                rank_1_3_within_90 += 1\n",
    "            total_lines += 1\n",
    "    return rank_1_within_90, rank_1_3_within_90, total_lines\n",
    "\n",
    "def main():\n",
    "    directory_path = \"output/\"\n",
    "    pattern = r\"places_test_pap_def_places_([^\\.]+)\\.jsonl\"\n",
    "    print(\"Model\\tRank 1 within 90% Levenshtein\\tRank 1-3 within 90% Levenshtein\")\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if re.match(pattern, filename):\n",
    "            match = re.match(pattern, filename)\n",
    "            model = match.group(1)\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            rank_1_within_90, rank_1_3_within_90, total_lines = process_file(file_path)\n",
    "            rank_1_within_90_percentage = (rank_1_within_90 / total_lines) * 100 if total_lines > 0 else 0\n",
    "            rank_1_3_within_90_percentage = (rank_1_3_within_90 / total_lines) * 100 if total_lines > 0 else 0\n",
    "            print(f\"{model}\\t{rank_1_within_90_percentage:.1f}%\\t{rank_1_3_within_90_percentage:.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71ccec48-0c66-44d1-8fe1-fd76df6bf568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tAverage Accuracy\tMedian Accuracy\n",
      "PHI_3_NO_INPUT\t40.54 years\t5.00 years\n",
      "DDbDPPHI_3_NO_INPUT\t40.84 years\t6.00 years\n",
      "exact_PHI_historian_1\t26.22 years\t1.00 years\n",
      "exact_allTies\t2239.32 years\t48.00 years\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def calculate_date_range(date):\n",
    "    \"\"\"Calculate the span (range) for the given date format.\"\"\"\n",
    "    match = re.match(r\"(-?\\d+)±(\\d+)\", date)\n",
    "    if match:\n",
    "        base, margin = int(match.group(1)), int(match.group(2))\n",
    "        return base - margin, base + margin\n",
    "    elif date.endswith('+'):\n",
    "        base = int(date[:-1])\n",
    "        return base, base + 25\n",
    "    elif date.endswith('-'):\n",
    "        base = int(date[:-1])\n",
    "        return base - 25, base\n",
    "    return None\n",
    "\n",
    "def clean_prediction(prediction):\n",
    "    \"\"\"Clean the prediction text to remove unwanted characters and convert to integer.\"\"\"\n",
    "    # Remove newline characters, unwanted text, and strip leading/trailing whitespace\n",
    "    cleaned = prediction.replace(\"\\n\", \"\").replace(\"assistant\", \"\").strip()\n",
    "    try:\n",
    "        # Convert cleaned prediction format to integer\n",
    "        return convert_prediction_format(cleaned)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def convert_prediction_format(prediction):\n",
    "    \"\"\"Convert prediction to the desired format.\"\"\"\n",
    "    # Match and convert prediction format\n",
    "    match = re.match(r\"(-?\\d+)±\\d+\", prediction)  # Match \"number±number\"\n",
    "    if match:\n",
    "        return int(match.group(1))  # Convert to base number\n",
    "\n",
    "    match = re.match(r\"(-?\\d+)\\+\", prediction)  # Match \"number+\"\n",
    "    if match:\n",
    "        return int(match.group(1)) + 12  # Convert to number + 12\n",
    "\n",
    "    match = re.match(r\"(-?\\d+)\\-\", prediction)  # Match \"number-\"\n",
    "    if match:\n",
    "        return int(match.group(1)) - 12  # Convert to number - 12\n",
    "\n",
    "    # For plain numbers, try to convert directly\n",
    "    try:\n",
    "        return int(prediction)\n",
    "    except ValueError:\n",
    "        return None  # Return None if conversion fails\n",
    "\n",
    "def calculate_distance(real_range, predicted):\n",
    "    \"\"\"Calculate the distance between the predicted date and the real date range.\"\"\"\n",
    "    real_start, real_end = real_range\n",
    "\n",
    "    if predicted is None:\n",
    "        return None\n",
    "\n",
    "    # Predicted is now an integer\n",
    "    if real_start <= predicted <= real_end:\n",
    "        return 0  # Within range\n",
    "    return min(abs(real_start - predicted), abs(real_end - predicted))\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"Process a single JSONL file and return the list of distances.\"\"\"\n",
    "    distances = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                real_response = entry[\"real_response\"]\n",
    "                rank_1_prediction = entry[\"predictions\"][\"rank_1\"]\n",
    "\n",
    "                # Calculate the real range\n",
    "                real_range = calculate_date_range(real_response)\n",
    "                if real_range is None:\n",
    "                    continue\n",
    "\n",
    "                # Clean and convert rank 1 prediction to integer\n",
    "                rank_1_prediction_cleaned = clean_prediction(rank_1_prediction)\n",
    "\n",
    "                # Calculate the distance for the rank 1 prediction\n",
    "                distance = calculate_distance(real_range, rank_1_prediction_cleaned)\n",
    "                if distance is not None:\n",
    "                    distances.append(distance)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON line: {line.strip()}\")\n",
    "                continue\n",
    "\n",
    "    return distances\n",
    "\n",
    "def main():\n",
    "    # Directory containing the JSONL files\n",
    "    directory_path = \"output/\"\n",
    "    pattern = r\"dates_test_inscr_def_dates_([^\\.]+)\\.jsonl\"\n",
    "\n",
    "    # Print the header for the tab-separated values\n",
    "    print(\"Model\\tAverage Accuracy\\tMedian Accuracy\")\n",
    "\n",
    "    # Iterate through the files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if re.match(pattern, filename):\n",
    "            # Extract the model from the filename\n",
    "            match = re.match(pattern, filename)\n",
    "            model = match.group(1)  # Extract only the model part\n",
    "\n",
    "            # Process the file and calculate distances\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            distances = process_file(file_path)\n",
    "\n",
    "            # Calculate average and median accuracy\n",
    "            if distances:\n",
    "                avg_accuracy = np.mean(distances)\n",
    "                median_accuracy = np.median(distances)\n",
    "\n",
    "                # Print results in tab-separated format\n",
    "                print(f\"{model}\\t{avg_accuracy:.2f} years\\t{median_accuracy:.2f} years\")\n",
    "            else:\n",
    "                print(f\"{model}\\tNo valid data\\tNo valid data\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c856aecc-2f2b-4e1f-a66e-b9b2d7deb16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tAverage Accuracy\tMedian Accuracy\n",
      "DDbDPPHI_3_NO_INPUT\t22.11 years\t0.00 years\n",
      "exact_DDbDP_historian_1\t21.72 years\t0.00 years\n",
      "exact_allTies\t7057.73 years\t14.00 years\n",
      "DDbDP_4_NO_INPUT\t23.89 years\t0.00 years\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def calculate_date_range(date):\n",
    "    \"\"\"Calculate the span (range) for the given date format.\"\"\"\n",
    "    match = re.match(r\"(-?\\d+)±(\\d+)\", date)\n",
    "    if match:\n",
    "        base, margin = int(match.group(1)), int(match.group(2))\n",
    "        return base - margin, base + margin\n",
    "    elif date.endswith('+'):\n",
    "        base = int(date[:-1])\n",
    "        return base, base + 25\n",
    "    elif date.endswith('-'):\n",
    "        base = int(date[:-1])\n",
    "        return base - 25, base\n",
    "    return None\n",
    "\n",
    "def clean_prediction(prediction):\n",
    "    \"\"\"Clean the prediction text to remove unwanted characters and convert to integer.\"\"\"\n",
    "    # Remove newline characters, unwanted text, and strip leading/trailing whitespace\n",
    "    cleaned = prediction.replace(\"\\n\", \"\").replace(\"assistant\", \"\").strip()\n",
    "    try:\n",
    "        # Convert cleaned prediction format to integer\n",
    "        return convert_prediction_format(cleaned)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def convert_prediction_format(prediction):\n",
    "    \"\"\"Convert prediction to the desired format.\"\"\"\n",
    "    # Match and convert prediction format\n",
    "    match = re.match(r\"(-?\\d+)±\\d+\", prediction)  # Match \"number±number\"\n",
    "    if match:\n",
    "        return int(match.group(1))  # Convert to base number\n",
    "\n",
    "    match = re.match(r\"(-?\\d+)\\+\", prediction)  # Match \"number+\"\n",
    "    if match:\n",
    "        return int(match.group(1)) + 12  # Convert to number + 12\n",
    "\n",
    "    match = re.match(r\"(-?\\d+)\\-\", prediction)  # Match \"number-\"\n",
    "    if match:\n",
    "        return int(match.group(1)) - 12  # Convert to number - 12\n",
    "\n",
    "    # For plain numbers, try to convert directly\n",
    "    try:\n",
    "        return int(prediction)\n",
    "    except ValueError:\n",
    "        return None  # Return None if conversion fails\n",
    "\n",
    "def calculate_distance(real_range, predicted):\n",
    "    \"\"\"Calculate the distance between the predicted date and the real date range.\"\"\"\n",
    "    real_start, real_end = real_range\n",
    "\n",
    "    if predicted is None:\n",
    "        return None\n",
    "\n",
    "    # Predicted is now an integer\n",
    "    if real_start <= predicted <= real_end:\n",
    "        return 0  # Within range\n",
    "    return min(abs(real_start - predicted), abs(real_end - predicted))\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"Process a single JSONL file and return the list of distances.\"\"\"\n",
    "    distances = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                real_response = entry[\"real_response\"]\n",
    "                rank_1_prediction = entry[\"predictions\"][\"rank_1\"]\n",
    "\n",
    "                # Calculate the real range\n",
    "                real_range = calculate_date_range(real_response)\n",
    "                if real_range is None:\n",
    "                    continue\n",
    "\n",
    "                # Clean and convert rank 1 prediction to integer\n",
    "                rank_1_prediction_cleaned = clean_prediction(rank_1_prediction)\n",
    "\n",
    "                # Calculate the distance for the rank 1 prediction\n",
    "                distance = calculate_distance(real_range, rank_1_prediction_cleaned)\n",
    "                if distance is not None:\n",
    "                    distances.append(distance)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON line: {line.strip()}\")\n",
    "                continue\n",
    "\n",
    "    return distances\n",
    "\n",
    "def main():\n",
    "    # Directory containing the JSONL files\n",
    "    directory_path = \"output/\"\n",
    "    pattern = r\"dates_test_pap_def_dates_([^\\.]+)\\.jsonl\"\n",
    "\n",
    "    # Print the header for the tab-separated values\n",
    "    print(\"Model\\tAverage Accuracy\\tMedian Accuracy\")\n",
    "\n",
    "    # Iterate through the files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if re.match(pattern, filename):\n",
    "            # Extract the model from the filename\n",
    "            match = re.match(pattern, filename)\n",
    "            model = match.group(1)  # Extract only the model part\n",
    "\n",
    "            # Process the file and calculate distances\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            distances = process_file(file_path)\n",
    "\n",
    "            # Calculate average and median accuracy\n",
    "            if distances:\n",
    "                avg_accuracy = np.mean(distances)\n",
    "                median_accuracy = np.median(distances)\n",
    "\n",
    "                # Print results in tab-separated format\n",
    "                print(f\"{model}\\t{avg_accuracy:.2f} years\\t{median_accuracy:.2f} years\")\n",
    "            else:\n",
    "                print(f\"{model}\\tNo valid data\\tNo valid data\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ed04311-8f7d-4149-91fd-b2fba4dab0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tCER\tTop-1 Accuracy\tTop-20 Accuracy\n",
      "3_DDbDP-minemerged1105_reconstructer_1\t28.9%\t55.0%\t71.0%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r\"[0· ]\", \"\", text)\n",
    "\n",
    "def truncate_to_length(text, target_length):\n",
    "    valid_text = re.sub(r\"[0· ]\", \"\", text)\n",
    "    if len(valid_text) <= target_length:\n",
    "        return valid_text\n",
    "    truncated_valid_text = valid_text[:target_length]\n",
    "    reconstructed_text = \"\"\n",
    "    valid_count = 0\n",
    "    for char in text:\n",
    "        if char not in \"0· \":\n",
    "            if valid_count < target_length:\n",
    "                reconstructed_text += char\n",
    "                valid_count += 1\n",
    "        else:\n",
    "            reconstructed_text += char\n",
    "    return reconstructed_text\n",
    "\n",
    "def calculate_cer(real, predicted):\n",
    "    real_normalized = normalize_text(real)\n",
    "    predicted_normalized = normalize_text(predicted)\n",
    "    return (1 - SequenceMatcher(None, real_normalized, predicted_normalized).ratio()) * 100\n",
    "\n",
    "def process_file(file_path):\n",
    "    total_cer = 0\n",
    "    rank_1_matches = 0\n",
    "    real_in_top_20 = 0\n",
    "    total_lines = 0\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            real_response = entry[\"real_response\"]\n",
    "            rank_1_prediction = entry[\"predictions\"][\"rank_1\"]\n",
    "            real_response_normalized = normalize_text(real_response)\n",
    "            target_length = len(real_response_normalized)\n",
    "            rank_1_prediction_truncated = truncate_to_length(rank_1_prediction, target_length)\n",
    "            cer = calculate_cer(real_response, rank_1_prediction_truncated)\n",
    "            total_cer += cer\n",
    "            if real_response_normalized == normalize_text(rank_1_prediction_truncated):\n",
    "                rank_1_matches += 1\n",
    "            predictions = [truncate_to_length(pred, target_length) for pred in entry[\"predictions\"].values()]\n",
    "            predictions_normalized = [normalize_text(pred) for pred in predictions]\n",
    "            if real_response_normalized in predictions_normalized:\n",
    "                real_in_top_20 += 1\n",
    "            total_lines += 1\n",
    "    avg_cer = total_cer / total_lines\n",
    "    rank_1_match_percentage = (rank_1_matches / total_lines) * 100\n",
    "    real_in_top_20_percentage = (real_in_top_20 / total_lines) * 100\n",
    "    return avg_cer, rank_1_match_percentage, real_in_top_20_percentage\n",
    "\n",
    "def main():\n",
    "    directory_path = \"ups_output_3/\"\n",
    "    pattern = r\"eds_ups18_([^\\.]+)\\.jsonl\"\n",
    "    print(\"Model\\tCER\\tTop-1 Accuracy\\tTop-20 Accuracy\")\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if re.match(pattern, filename):\n",
    "            match = re.match(pattern, filename)\n",
    "            model = match.group(1)\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            avg_cer, rank_1_match_percentage, real_in_top_20_percentage = process_file(file_path)\n",
    "            print(f\"{model}\\t{avg_cer:.1f}%\\t{rank_1_match_percentage:.1f}%\\t{real_in_top_20_percentage:.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a434081-a8ac-4e50-a851-ec333add0c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tCER\tTop-1 Accuracy\tTop-20 Accuracy\n",
      "DDbDP-minemerged1105_reconstructer_1\t17.2%\t62.0%\t84.0%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r\"[0· ]\", \"\", text)\n",
    "\n",
    "def truncate_to_length(text, target_length):\n",
    "    valid_text = re.sub(r\"[0· ]\", \"\", text)\n",
    "    if len(valid_text) <= target_length:\n",
    "        return valid_text\n",
    "    truncated_valid_text = valid_text[:target_length]\n",
    "    reconstructed_text = \"\"\n",
    "    valid_count = 0\n",
    "    for char in text:\n",
    "        if char not in \"0· \":\n",
    "            if valid_count < target_length:\n",
    "                reconstructed_text += char\n",
    "                valid_count += 1\n",
    "        else:\n",
    "            reconstructed_text += char\n",
    "    return reconstructed_text\n",
    "\n",
    "def calculate_cer(real, predicted):\n",
    "    real_normalized = normalize_text(real)\n",
    "    predicted_normalized = normalize_text(predicted)\n",
    "    return (1 - SequenceMatcher(None, real_normalized, predicted_normalized).ratio()) * 100\n",
    "\n",
    "def process_file(file_path):\n",
    "    total_cer = 0\n",
    "    rank_1_matches = 0\n",
    "    real_in_top_20 = 0\n",
    "    total_lines = 0\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            real_response = entry[\"real_response\"]\n",
    "            rank_1_prediction = entry[\"predictions\"][\"rank_1\"]\n",
    "            real_response_normalized = normalize_text(real_response)\n",
    "            target_length = len(real_response_normalized)\n",
    "            rank_1_prediction_truncated = truncate_to_length(rank_1_prediction, target_length)\n",
    "            cer = calculate_cer(real_response, rank_1_prediction_truncated)\n",
    "            total_cer += cer\n",
    "            if real_response_normalized == normalize_text(rank_1_prediction_truncated):\n",
    "                rank_1_matches += 1\n",
    "            predictions = [truncate_to_length(pred, target_length) for pred in entry[\"predictions\"].values()]\n",
    "            predictions_normalized = [normalize_text(pred) for pred in predictions]\n",
    "            if real_response_normalized in predictions_normalized:\n",
    "                real_in_top_20 += 1\n",
    "            total_lines += 1\n",
    "    avg_cer = total_cer / total_lines\n",
    "    rank_1_match_percentage = (rank_1_matches / total_lines) * 100\n",
    "    real_in_top_20_percentage = (real_in_top_20 / total_lines) * 100\n",
    "    return avg_cer, rank_1_match_percentage, real_in_top_20_percentage\n",
    "\n",
    "def main():\n",
    "    directory_path = \"ups_output_106/\"\n",
    "    pattern = r\"eds_ups106_1_([^\\.]+)\\.jsonl\"\n",
    "    print(\"Model\\tCER\\tTop-1 Accuracy\\tTop-20 Accuracy\")\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if re.match(pattern, filename):\n",
    "            match = re.match(pattern, filename)\n",
    "            model = match.group(1)\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            avg_cer, rank_1_match_percentage, real_in_top_20_percentage = process_file(file_path)\n",
    "            print(f\"{model}\\t{avg_cer:.1f}%\\t{rank_1_match_percentage:.1f}%\\t{real_in_top_20_percentage:.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7f5931-dd12-4dc8-8e9c-ac6c557d0c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tCER\tTop-1 Accuracy\tTop-20 Accuracy\n",
      "quantized_DDbDP_text\t16.3%\t71.2%\t84.1%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r\"[0· ]\", \"\", text)\n",
    "\n",
    "def truncate_to_length(text, target_length):\n",
    "    valid_text = re.sub(r\"[0· ]\", \"\", text)\n",
    "    if len(valid_text) <= target_length:\n",
    "        return valid_text\n",
    "    truncated_valid_text = valid_text[:target_length]\n",
    "    reconstructed_text = \"\"\n",
    "    valid_count = 0\n",
    "    for char in text:\n",
    "        if char not in \"0· \":\n",
    "            if valid_count < target_length:\n",
    "                reconstructed_text += char\n",
    "                valid_count += 1\n",
    "        else:\n",
    "            reconstructed_text += char\n",
    "    return reconstructed_text\n",
    "\n",
    "def calculate_cer(real, predicted):\n",
    "    real_normalized = normalize_text(real)\n",
    "    predicted_normalized = normalize_text(predicted)\n",
    "    return (1 - SequenceMatcher(None, real_normalized, predicted_normalized).ratio()) * 100\n",
    "\n",
    "def process_file(file_path):\n",
    "    total_cer = 0\n",
    "    rank_1_matches = 0\n",
    "    real_in_top_20 = 0\n",
    "    total_lines = 0\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            real_response = entry[\"real_response\"]\n",
    "            rank_1_prediction = entry[\"predictions\"][\"rank_1\"]\n",
    "            real_response_normalized = normalize_text(real_response)\n",
    "            target_length = len(real_response_normalized)\n",
    "            rank_1_prediction_truncated = truncate_to_length(rank_1_prediction, target_length)\n",
    "            cer = calculate_cer(real_response, rank_1_prediction_truncated)\n",
    "            total_cer += cer\n",
    "            if real_response_normalized == normalize_text(rank_1_prediction_truncated):\n",
    "                rank_1_matches += 1\n",
    "            predictions = [truncate_to_length(pred, target_length) for pred in entry[\"predictions\"].values()]\n",
    "            predictions_normalized = [normalize_text(pred) for pred in predictions]\n",
    "            if real_response_normalized in predictions_normalized:\n",
    "                real_in_top_20 += 1\n",
    "            total_lines += 1\n",
    "    avg_cer = total_cer / total_lines\n",
    "    rank_1_match_percentage = (rank_1_matches / total_lines) * 100\n",
    "    real_in_top_20_percentage = (real_in_top_20 / total_lines) * 100\n",
    "    return avg_cer, rank_1_match_percentage, real_in_top_20_percentage\n",
    "\n",
    "def main():\n",
    "    directory_path = \"quant/\"\n",
    "    pattern = r\"eds_test_pap_def_eds_fixed_([^\\.]+)\\.jsonl\"\n",
    "    print(\"Model\\tCER\\tTop-1 Accuracy\\tTop-20 Accuracy\")\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if re.match(pattern, filename):\n",
    "            match = re.match(pattern, filename)\n",
    "            model = match.group(1)  # Extract only the model part\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            avg_cer, rank_1_match_percentage, real_in_top_20_percentage = process_file(file_path)\n",
    "            print(f\"{model}\\t{avg_cer:.1f}%\\t{rank_1_match_percentage:.1f}%\\t{real_in_top_20_percentage:.1f}%\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff494ca6-2ed7-4e7b-a196-f4d25177579b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\tCER\tTop-1 Accuracy\tTop-20 Accuracy\n",
      "DDbDP-minemerged1105_reconstructer_1\t16.3%\t71.3%\t85.0%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r\"[0· ]\", \"\", text)\n",
    "\n",
    "def truncate_to_length(text, target_length):\n",
    "    valid_text = re.sub(r\"[0· ]\", \"\", text)\n",
    "    if len(valid_text) <= target_length:\n",
    "        return valid_text\n",
    "    truncated_valid_text = valid_text[:target_length]\n",
    "    reconstructed_text = \"\"\n",
    "    valid_count = 0\n",
    "    for char in text:\n",
    "        if char not in \"0· \":\n",
    "            if valid_count < target_length:\n",
    "                reconstructed_text += char\n",
    "                valid_count += 1\n",
    "        else:\n",
    "            reconstructed_text += char\n",
    "    return reconstructed_text\n",
    "\n",
    "def calculate_cer(real, predicted):\n",
    "    real_normalized = normalize_text(real)\n",
    "    predicted_normalized = normalize_text(predicted)\n",
    "    return (1 - SequenceMatcher(None, real_normalized, predicted_normalized).ratio()) * 100\n",
    "\n",
    "def process_file(file_path):\n",
    "    total_cer = 0\n",
    "    rank_1_matches = 0\n",
    "    real_in_top_20 = 0\n",
    "    total_lines = 0\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            real_response = entry[\"real_response\"]\n",
    "            rank_1_prediction = entry[\"predictions\"][\"rank_1\"]\n",
    "            real_response_normalized = normalize_text(real_response)\n",
    "            target_length = len(real_response_normalized)\n",
    "            rank_1_prediction_truncated = truncate_to_length(rank_1_prediction, target_length)\n",
    "            cer = calculate_cer(real_response, rank_1_prediction_truncated)\n",
    "            total_cer += cer\n",
    "            if real_response_normalized == normalize_text(rank_1_prediction_truncated):\n",
    "                rank_1_matches += 1\n",
    "            predictions = [truncate_to_length(pred, target_length) for pred in entry[\"predictions\"].values()]\n",
    "            predictions_normalized = [normalize_text(pred) for pred in predictions]\n",
    "            if real_response_normalized in predictions_normalized:\n",
    "                real_in_top_20 += 1\n",
    "            total_lines += 1\n",
    "    avg_cer = total_cer / total_lines\n",
    "    rank_1_match_percentage = (rank_1_matches / total_lines) * 100\n",
    "    real_in_top_20_percentage = (real_in_top_20 / total_lines) * 100\n",
    "    return avg_cer, rank_1_match_percentage, real_in_top_20_percentage\n",
    "\n",
    "def main():\n",
    "    directory_path = \"output_efter_fix/\"\n",
    "    pattern = r\"eds_test_pap_def_eds_fixed_([^\\.]+)\\.jsonl\"\n",
    "    print(\"Model\\tCER\\tTop-1 Accuracy\\tTop-20 Accuracy\")\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if re.match(pattern, filename):\n",
    "            match = re.match(pattern, filename)\n",
    "            model = match.group(1)  # Extract only the model part\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            avg_cer, rank_1_match_percentage, real_in_top_20_percentage = process_file(file_path)\n",
    "            print(f\"{model}\\t{avg_cer:.1f}%\\t{rank_1_match_percentage:.1f}%\\t{real_in_top_20_percentage:.1f}%\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8b81b5-a79e-4b55-b27b-ac73970d778c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5661fb-f6d0-4a97-80b1-22074918ac16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7fdf5a-7fe3-4ad0-bb67-1d9d96333b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MarcoPolo",
   "language": "python",
   "name": "eric_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
